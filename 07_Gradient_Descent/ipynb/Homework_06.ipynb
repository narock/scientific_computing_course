{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "## Looking at Gradient Descent More Indepth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following two functions implement the Gradient Descent algorithm for a linear fit to data (modeling your data using a straight line). The mse function is used internally by the gradient_descent function, and does not need to be called directly. gradient_descent takes 6 inputs: x_observations, y_observations, epochs (number of interations to perform before stopping), step size, starting value of m (slope), and starting value of b (y-intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute Mean Squared Error\n",
    "def mse(y_current, y_obs, N):\n",
    "    m = np.array( (y_obs-y_current)**2 )\n",
    "    m = m.sum()/N\n",
    "    return m\n",
    "\n",
    "# function to implement gradient descent\n",
    "def gradient_descent(x_obs, y_obs, epochs, step_size, m_current, b_current):\n",
    "    best_cost = float('inf')\n",
    "    best_m = 0\n",
    "    best_b = 0\n",
    "    N = float(len(y_obs))\n",
    "    for i in range(epochs):\n",
    "        # use the current slope and y-intercept to define a straight line\n",
    "        y_current = (m_current * x_obs) + b_current\n",
    "        # compute the MSE of the above line\n",
    "        cost = mse(y_current, y_obs, N)\n",
    "        # did we improve? If the 'cost' (the MSE value) is less than what we've seen \n",
    "        # previously then mark this m and b as our current best option\n",
    "        if ( cost < best_cost ):\n",
    "            best_cost = cost\n",
    "            best_m = m_current\n",
    "            best_b = b_current\n",
    "        # which direction, and by how much, should we change m and b?\n",
    "        # compute the gradient to find out\n",
    "        m_gradient = -(2./N) * sum(x_obs * (y_obs - y_current))\n",
    "        b_gradient = -(2./N) * sum(y_obs - y_current)\n",
    "        # update m and b based on the gradient and the step size\n",
    "        m_current = m_current - (step_size * m_gradient)\n",
    "        b_current = b_current - (step_size * b_gradient)\n",
    "    return best_m, best_b, best_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell reads and formats our web site usage data from class. We will reuse this data throughout the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries we'll need\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read the web site traffic data using Pandas\n",
    "#url = 'https://narock.github.io/teaching/CST-411/web_site.tsv'\n",
    "#data = pd.read_csv(url, sep=\"\\t\")\n",
    "data = pd.read_csv('web_site.tsv', sep='\\t')\n",
    "\n",
    "# set up the data as numpy arrays instead of Pandas table\n",
    "# this will make the computations easier\n",
    "x_obs = data['Days'].tolist()\n",
    "y_obs = data['Time'].tolist()\n",
    "x_obs = np.array( x_obs )\n",
    "y_obs = np.array( y_obs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the output of the Gradient Descent algorithm impacted by where we start searching? In other words, what impact, if any, do our initial guesses of m and b have on the results? We'll investigate this through the next set of questions.\n",
    "\n",
    "1.) (9 points) Set up a loop that calls gradient_descent 100 times. Each time through the loop use random starting values for m and b. Your random values should be in the range [-5,5). Use an epoch of 5000 and a step size of 0.00001. Keep track of 100 m, b, and cost function results. <b>(Note that this cell may take several seconds to execute. Make sure it has completed before moving on).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) (4 points) Create a scatter plot of the 100 m and b values from problem 1. Place m on the x-axis\n",
    "and b on the y-axis. Label both axes and provide a title for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.) (1 point) Create a histogram of the 100 cost function values returned from problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.) (3 points) Do m and b seem to be impacted by where we start our search? Does one parameter (m or b) seem to be more impacted than the other, or are they both equally impacted? What are your thoughts on why this might be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.) (6 points) Now we'll look at the impact of step size on the algorithm. We'll keep the initial guess for m and b constant. Use 0 as an initial guess for both m and b. Set the epoch value to 5000. Use these values to execute the gradient_descent function with step size values of 0.01, 0.001, 0.0001, and 0.00001. Keep track of the m, b, and cost function results from all four executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.) (4 points) Create a scatter plot of the m and b values that you obtained in problem 5. Place m on the x-axis\n",
    "and b on the y-axis. Label both axes and provide a title for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Extra Credit (3 points)</b> - recreate the plot from 6. but this time label the points so we know which point goes with which step size. You can label the points using colors, text labels, or both. Your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.) (3 points) Summarize the impacts of step size and starting location on the results of Gradient Descent. Do you feel that one is has more of an impact than the other? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we improve our results any by not being so random in how we initially choose m and b? Here we're looking at web site traffic data. We have data on how long someone has been a member of the site and how long they stay on the site. Let's say we research other web sites with similar functionality. Our research indicates that the 'best' fit y-intercept (b) is usually between -0.5 and 0.5. Will restricting the starting value of b to be between -0.5 and 0.5 have any impact on the results we get? \n",
    "\n",
    "8.) (9 points) Repeat problem 1. but this time restrict your random guess of b to be between -0.5 and 0.5 instead of -5 and 5. Keep your random guess of m to between -5 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.) (4 points) Create a scatter plot of the 100 m and b values from problem 8. Place m on the x-axis\n",
    "and b on the y-axis. Label both axes and provide a title for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.) (1 point) Create a histogram of the 100 cost function values returned from problem 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.) (2 points) What impact, if any, did this have on the results of our Gradient Descent algorithm? Was applying background/domain information helpful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the optimal step size to use? What should we do if we don't have background/domain information to guide our initial guesses of m and b? This is currently an active area of research in machine learning. There are some tips and guidelines; however, in general, it's still an open question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.) (4 points) One thing reseachers and practitioners are doing is working with a variable step size. The idea is to gradually decrease the step size as you move through the epochs. The thinking is that we take larger steps in the beginning as we're finding our way and we take smaller steps towards the end as we're, hopefully, closing in on the solution.\n",
    "\n",
    "Copy and paste the gradient_descent function into the cell below. Rename it to be gradient_descent2. Modify gradient_descent2 so that the step size decreases as you move through the epochs. How much the step size decreases and when is up to you. Try implementing different approaches and see what impact it has on the results. Are you able to obtain a lower cost function value using a variable step size? If you're not, don't worry, this problem is more focused on how you would update the algorithm to implement this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
